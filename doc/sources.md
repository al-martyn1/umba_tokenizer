# Brief Description for Project Sources

- [Информация по исходным текстам](#информация-по-исходным-текстам)
- [Состав и содержимое исходников](#состав-и-содержимое-исходников)
  - [inc/umba](#incumba)
  - [inc/umba/tokenizer](#incumbatokenizer)
  - [inc/umba/tokenizer/filters](#incumbatokenizerfilters)
  - [inc/umba/tokenizer/lexers](#incumbatokenizerlexers)
  - [inc/umba/tokenizer/parsers/mermaid](#incumbatokenizerparsersmermaid)
  - [inc/umba/tokenizer/parsers](#incumbatokenizerparsers)
  - [inc/umba/tokenizer/parsers/ufsm](#incumbatokenizerparsersufsm)
  - [inc/umba/tokenizer/parsers/ufsm/samples](#incumbatokenizerparsersufsmsamples)
  - [inc/umba/tokenizer/parsers/ufsm](#incumbatokenizerparsersufsm-1)
  - [inc/umba/tokenizer/parsers](#incumbatokenizerparsers-1)
  - [inc/umba/tokenizer](#incumbatokenizer-1)
  - [inc/umba](#incumba-1)
- [Предупреждения](#предупреждения)
  - [inc/umba/tokenizer](#incumbatokenizer-2)
- [Обратить внимание](#обратить-внимание)
  - [inc/umba](#incumba-2)
  - [inc/umba/tokenizer](#incumbatokenizer-3)
  - [inc/umba/tokenizer/filters](#incumbatokenizerfilters-1)
  - [inc/umba/tokenizer/lexers](#incumbatokenizerlexers-1)
  - [inc/umba/tokenizer/parsers/mermaid](#incumbatokenizerparsersmermaid-1)
- [Заметки](#заметки)
  - [inc/umba/tokenizer](#incumbatokenizer-4)
  - [inc/umba/tokenizer/parsers/ufsm](#incumbatokenizerparsersufsm-2)
- [To do](#to-do)
  - [inc/umba/tokenizer](#incumbatokenizer-5)
  - [inc/umba/tokenizer/filters](#incumbatokenizerfilters-2)

# Информация по исходным текстам

# Состав и содержимое исходников


## inc/umba

- `c_char_class.h` - Константы классов символов. При помощи классов символов можно просто писать простейшие лексера. Уложены в 16 бит для компактности для встраиваемых применений.
- `c_char_class_table.h.inc` - Таблица классов символов для симолов базовой таблицы ASCII, версия "C"
- `c_tokenizer.h` - Утилиты для работы с trie символов. Предполагалось делать с поддержкой языка "C"
- `c_tokenizer_constants.h` - "Стандартные" предопределенные константы для токенов. Используются макросы, так как предполагается расширение набора констант, и предполагалось использование из языка "C"
- `char_class.h` - Константы классов символов (C++). При помощи классов символов можно просто писать простейшие лексера
- `char_class_table.h.inc` - Таблица классов символов для симолов базовой таблицы ASCII, версия "C++"
- `tokenizer.h` - Основной файл библиотеки токенизатора. Подключает все необходимые заголовки

## inc/umba/tokenizer

- `defs.h` - Общие макросы, а также определение/задание опций сборки. Внутренний хидер.
- `enums.h` - Autogenerated enumerations (Umba Enum Gen)
- `exceptions.h` - Исключения для marty::format. Стиль именов - snake_case, так как мы наследуемся от стандартной библиотеки

## inc/umba/tokenizer/filters

- `cc_preprocessor_filter.h` - Фильтр, вставляющий маркеры (без полезной нагрузки) препроцессора C/C++. Переключает режим угловых скобок, чтобы их содержимое парсилось как строка. Переключает режим символа '#'.
- `dbl_square_brackets_composing_filters.h` - Фильтры DblSquareBracketOpenComposingFilter и DblSquareBracketCloseComposingFilter - преобразуют двойные токены [[ и ]] в одинарные
- `filter_base.h` - Базовый класс для сложных фильтров с буферизацией токенов
- `identifier_to_keyword_conversion_filter.h` - Фильтр для преобразования идентификаторов в различные токены по заданной таблице
- `kebab_case_composing_filter.h` - Филтр для преобразования последовательностей идентификаторов и неразрывных с ними минусов в единственный kebab-style идентификатор
- `repeated_token_composing_filter.h` - Фильтр для преобразования простой последовательности из N токенов в единственный токен
- `simple_pass_trough_filter.h` - Фильтр, просто пропускающий токены дальше. Пример простого фильтра
- `simple_replace_filter.h` - Фильтр простой замены одного токена на другой. Нагрузка не меняется
- `simple_sequence_composing_filter.h` - Фильтр замены последовательности токенов на один токен
- `simple_suffix_gluing_filter.h` - Фильтр, "приклеивающий" идентификатор-суффикс к строковому или числовому литералу
- `token_collecting_filter.h` - Простой фильтр, который буферизирует одинаковые токены. Он хранит только итератор первого токена в последовательности
- `token_range_conversion_filter.h` - Конвертирует токены из диапазона в один токен, и устанавливает нагрузку в размер офсета от стартового, добавяя дополнительное смещение. Предназначено для операторов переменной длины
- `unclassified_chars_collecting_filter.h` - Фильтр, склеивающий отдельные неклассифицированные символы в последовательность

## inc/umba/tokenizer/lexers

- `cpp.h` - Лексер (сканер) для языка C/C++
- `mermaid_packet_diagram.h` - Лексер (сканер) диаграмм mermaid packet diagram
- `mermaid_packet_diagram_tokens.h` - Определение токенов, специфичных для mermaid packet diagram
- `plantuml.h` - Лексер (сканер) диаграмм plantuml
- `ufsm.h` - Лексер (сканер) диаграмм UmbaFSM
- `ufsm_tokens.h` - Определение токенов, специфичных для Umba State Machine
- `usketch_tokenizer.h` - Токенизатор/лексер usketch
- `usketch_tokens.h` - Определение токенов, специфичных для usketch

## inc/umba/tokenizer/parsers/mermaid

- `packet_diagram_cpp.h` - Хелперы для вывода в сишечку и плюсики, а также в текстовые диаграммы
- `packet_diagram_enums.h` - Autogenerated enumerations (Umba Enum Gen)
- `packet_diagram_parser.h` - Парсер mermaid packet diagram, расширенная версия, с использованием типов и массивов
- `packet_diagram_parser_types.h` - Типы для парсера mermaid packet diagram, расширенная версия, с использованием типов и массивов

## inc/umba/tokenizer/parsers

- `mermaid_packet_diagram_parser_new.h` - Парсер mermaid packet diagram, расширенная версия, с использованием типов и массивов
- `parser_base.h` - Базовый парсер

## inc/umba/tokenizer/parsers/ufsm

- `basic_typedefs.h` - Базовые typedef'ы для парсера ufsm
- `basic_types.h` - Базовые типы для парсера ufsm
- `enums.h` - Autogenerated enumerations (Umba Enum Gen)
- `parser.h` - Парсер языка Umba FSM

## inc/umba/tokenizer/parsers/ufsm/samples

- `concept.ufsm` -
- `elevator.ufsm` -
- `traffic_lights.ufsm` -

## inc/umba/tokenizer/parsers/ufsm

- `types.h` - Типы для парсера ufsm

## inc/umba/tokenizer/parsers

- `usketch_parser.h` - Парсер USketch - простое рисование
- `usketch_parser_types.h` - Типы для парсера usketch
- `utils.h` - Утилиты

## inc/umba/tokenizer

- `string_literal_parsing.h` - Интерфейс парсера строковых литералов ITokenizerLiteralParser, реализации парсеров SimpleQuotedStringLiteralParser и CppEscapedSimpleQuotedStringLiteralParser. Внутренний хидер.
- `token_collection.h` - Коллекция токенов.
- `token_filters.h` - Сборник всех фильтров токенизера
- `tokenizer.h` - Шаблонный класс Tokenizer, фвляется typedef'ом для класса TokenizerFunctionHandlers, в котором используются обработчики на базе std::function. Внутренний хидер.
- `tokenizer_base.h` - Реализация токенизера. Внутренний хидер.
- `tokenizer_builder.h` - TokenizerBuilder - создание токенизера. Для создания поисковых trie в токенизере используются TrieBuilder'ы. Внутренний хидер.
- `tokenizer_function_handlers.h` - TokenizerFunctionHandlers - токенизатор с обработчиками std::function. Внутренний хидер.
- `tokenizer_log.h` - Подсистема протоколирования для токенизеров-парсеров
- `tokenizer_log_console.h` - Подсистема протоколирования для токенизеров-парсеров (вывод в консоль)
- `tokenizer_options.h` - TokenizerOptions - параметры работы токенизера. Внутренний хидер.
- `tpl.h` - Шаблон внутреннего хидера (не включаемого явно пользователем) библиотеки токенизатора. Внутренний хидер.
- `trie_builder.h` - TrieBuilder - построитель префиксного дерева trie. Для простоты хранит всё в map, при построении trie создаёт компактную структуру. Внутренний хидер.
- `types.h` - Типы данных для подсистемы токенизации
- `utils.h` - Утилиты. Внутренний хидер.

## inc/umba

- `undef_min_max.h` -


# Предупреждения


## inc/umba/tokenizer

- `[inc/umba/tokenizer/tokenizer_log.h:95]`
  /info report

- `[inc/umba/tokenizer/tokenizer_log.h:96]`
  /info report


# Обратить внимание


## inc/umba

- `[inc/umba/c_tokenizer_constants.h:76]`
  При изменении базовых констант не забываем, что нельзя вылезать за
  UMBA_TOKENIZER_TOKEN_BASE_LAST (или надо поправить эту константу)


## inc/umba/tokenizer

- `[inc/umba/tokenizer/string_literal_parsing.h:518]`
  В MSVC работает, в GCC надо через явное указание базы

- `[inc/umba/tokenizer/tokenizer_base.h:312]`
  Надо вспомнить, почему я сделал DataType##Holder, ведь изначально в variant'е
  были именно типы DataType Скорее всего, variant<DataType> был очень жирный, а
  также его копирование было дорогим. Но это не факт.

- `[inc/umba/tokenizer/tokenizer_base.h:764]`
  return'а не было, был забыт, а что и в каких случаях возращать - я уже не помню

- `[inc/umba/tokenizer/tokenizer_base.h:1221]`
  Тут надо преобразовать префикс в число, и поместить его в numberCurrentIntValue

- `[inc/umba/tokenizer/tokenizer_base.h:1238]`
  Не забыть передать numberCurrentIntValue

- `[inc/umba/tokenizer/tokenizer_base.h:1721]`
  Тут ещё надо проработать - а вдруг это 0 с каким-то суффиксом? по идее, всю
  цепочку итераторов надо бы хранить где-то, чтобы в данном случае сбросить её
  пользователю, а не выдавать ошибку

- `[inc/umba/tokenizer/tokenizer_base.h:1761]`
  тут надо уже начинать подсчитывать std::uint64_t numberCurrentIntValue

- `[inc/umba/tokenizer/tokenizer_base.h:1851]`
  Вот тут гавно какое-то получится, надо подумать

- `[inc/umba/tokenizer/tokenizer_base.h:1858]`
  Добавил

- `[inc/umba/tokenizer/tokenizer_base.h:1869]`
  Добавил

- `[inc/umba/tokenizer/tokenizer_base.h:1877]`
  unreachable code st = TokenizerInternalState::stReadNumberFloat;

- `[inc/umba/tokenizer/tokenizer_base.h:1891]`
  Вот тут гавно какое-то получится, надо подумать

- `[inc/umba/tokenizer/tokenizer_base.h:1925]`
  Вот тут гавно какое-то получится, надо подумать

- `[inc/umba/tokenizer/tokenizer_base.h:2336]`
  Пока IdentifierData задаем как вьюшку от итераторов, но вообще идентификатор
  надо бы сохранять в буфере, чтобы корректно обрабатывать linefeed escap'ы

- `[inc/umba/tokenizer/tokenizer_builder.h:223]`
  Чот какая-то херня непонятная сама ф-я addTokenToKnownSet выглядит так: void
  addTokenToKnownSet(payload_type tk, const string_type &tkStr, bool
  allowExisting=false) А тут мы string_type как-то очень странно конструируем.
  Вроде бы нам надо просто сделать строку из одного символа А тут мы вроде бы
  делаем строки размером bracketsPair[0] и bracketsPair[1] из символов
  pairBaseToken и pairBaseToken+1 Пока закоментим, и сделаем как надо
  addTokenToKnownSet(pairBaseToken  , string_type(bracketsPair[0], pairBaseToken
  )); addTokenToKnownSet(pairBaseToken+1, string_type(bracketsPair[1],
  pairBaseToken+1));

- `[inc/umba/tokenizer/token_collection.h:217]`
  У нас итераторы довольно жирные (32/64 байта), там овердофига всего хранится
  допустим, токенов у нас в 4 раза меньше, чем текста - если много кода, то много
  мелких токенов - операторов и тп но идентификаторы обычно длиннее 4х символов
  если есть коментарии или строковые литералы - то они разбавляют плотность

- `[inc/umba/tokenizer/token_collection.h:503]`
  Надо наверное что-то придумать с итератором конца. Или не надо? ,
  m_tokenizer(tknConfigurator(initTokenizerHandlers(std::move(tkn))))


## inc/umba/tokenizer/filters

- `[inc/umba/tokenizer/filters/simple_suffix_gluing_filter.h:137]`
  Зачем я тут копию делаю?


## inc/umba/tokenizer/lexers

- `[inc/umba/tokenizer/lexers/cpp.h:147]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/mermaid_packet_diagram.h:106]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/plantuml.h:178]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/ufsm.h:113]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/usketch_tokenizer.h:193]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/usketch_tokenizer.h:224]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/usketch_tokenizer.h:271]`
  Фильтры, установленные позже, отрабатывают раньше


## inc/umba/tokenizer/parsers/mermaid

- `[inc/umba/tokenizer/parsers/mermaid/packet_diagram_parser.h:1468]`
  Тут надо делать селект - если токен не числовой внутри орга, то литеральный
  глобальный Пока только числовой делаем

- `[inc/umba/tokenizer/parsers/mermaid/packet_diagram_parser.h:1490]`
  Надо разобраться, почему диагностика не туда пырит


# Заметки


## inc/umba/tokenizer

- `[inc/umba/tokenizer/tokenizer_base.h:859]`
  Надо ли semialpha проверять, не является ли она началом числового префикса?
  Наверное, не помешает

- `[inc/umba/tokenizer/tokenizer_base.h:1675]`
  У нас пока так: префикс числа начинается с любой цифры, потом могут следовать
  любые символы, после префикса - те символы, которые разрешены префиксом

- `[inc/umba/tokenizer/tokenizer_base.h:1757]`
  Да, сразу после префикса у нас не может быть разделителя разрядов


## inc/umba/tokenizer/parsers/ufsm

- `[inc/umba/tokenizer/parsers/ufsm/basic_types.h:39]`
  составное событие, хотя и записывается в виде логического выражения -
  конъюнкции, является не конъюнкцией, а просто списком, в котором разделителем
  выступает символ '|'.

- `[inc/umba/tokenizer/parsers/ufsm/basic_types.h:117]`
  заметка по терминологии. Дефиниции (`definitions`) полностью аналогичны
  автоматам, но из дефиниций нельзя произвести генерацию автомата в диаграмму или
  код. Автоматы можно переиспользовать (унаследовать) при помощи ключевого слова
  `inherits`, дефиниции переиспользуются при помощи ключевого слова `uses`.

- `[inc/umba/tokenizer/parsers/ufsm/basic_types.h:122]`
  Нужно продумать, как происходит формирование списков действий, событий,
  предикатов и переходов при наследовании `inherits` автоматов (и использовании
  `uses` дефиниций). Пока есть идея, что непосредственно описанные в автомате
  сущности первичны, а то, что мы поттягиваем из ранее определенных автоматов и
  дефиниций добавляется (и может переопределять сущности автомата), тогда
  выражение `override` в `inherits`/`uses` имеет смысл. Иначе же у нас сущности
  берутся из первой базы (но в принципе, `override` имеет смысл для последущих
  `inherits`/`uses`), и надо как-то уметь их переопределять в описании текущего
  автомата - для каждого раздела задавать override? в принципе, тоже рабочий
  вариант. Как же быть?


# To do


## inc/umba/tokenizer

- [ ] `[inc/umba/tokenizer/tokenizer_base.h:558]`
  Надо определится, что делать в tokenize в raw режиме. См там коментарии

- [ ] `[inc/umba/tokenizer/tokenizer_base.h:1257]`
  У нас был встречен символ разделяющий целую и дробную часть плавающего числа,
  но без целой части. Пока просто съедаем его, но вообще этот символ может быть
  оператором, надо это обработать Например, это может быть паскалевский `End.`
  (End с точкой)

- [ ] `[inc/umba/tokenizer/tokenizer_base.h:1296]`
  Надо уточнить, что за комент

- [ ] `[inc/umba/tokenizer/tokenizer_base.h:2070]`
  Разобраться с continuation

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:216]`
  Проверить, что pairBaseToken один из: UMBA_TOKENIZER_TOKEN_CURLY_BRACKETS
  UMBA_TOKENIZER_TOKEN_ROUND_BRACKETS UMBA_TOKENIZER_TOKEN_ANGLE_BRACKETS
  UMBA_TOKENIZER_TOKEN_SQUARE_BRACKETS или 0, для автоопределения, и сделать
  автоопределение

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:253]`
  Подумать, как сделать, чтобы числа можно было начинать с символов @ # $

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:254]`
  Проверить tokenId на вхождение в диапазон, или сделать автоопределение

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:271]`
  Проверить tokenId на вхождение в диапазон, или сделать автоопределение

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:315]`
  Проверить tokenId на вхождение в диапазон, или сделать автоопределение

- [ ] `[inc/umba/tokenizer/tokenizer_log.h:163]`
  Тут надо бы сделать итерацию по символам текста, а не по char'ам

- [ ] `[inc/umba/tokenizer/token_collection.h:155]`
  Ещё забыт вертикальный TAB

- [ ] `[inc/umba/tokenizer/token_collection.h:156]`
  Символы меньше пробела - сейчас по идее, просто пропускаются, наверное надо
  сделать опцию, чтобы они вызывали ошибку

- [ ] `[inc/umba/tokenizer/token_collection.h:268]`
  Надо переделать на unordered_map, чтобы можно было удалять элементы

- [ ] `[inc/umba/tokenizer/token_collection.h:298]`
  запретить все остальные конструкторы

- [ ] `[inc/umba/tokenizer/token_collection.h:299]`
  запретить копирование

- [ ] `[inc/umba/tokenizer/token_collection.h:751]`
  может не надо в getTokenImpl проверять доступность? Или пофик, одно условие,
  зато peekToken() упрощается

- [ ] `[inc/umba/tokenizer/token_collection.h:826]`
  Вот тут надо вычитать следующий токен из входного текста

- [ ] `[inc/umba/tokenizer/token_collection.h:873]`
  надо как-то просигналить, потому что снаружы мы генерим только ошибки
  синтаксического анализа а ошибки лексера генерируются им самим. Но раз лексер
  вернул true, то там ошибки не было, но, тем не менее, финальный токен не
  появился в массиве токенов - что-то пошло не так. unexpected-что?


## inc/umba/tokenizer/filters

- [ ] `[inc/umba/tokenizer/filters/cc_preprocessor_filter.h:42]`
  Вообще, по уму, надо бы сделать энум со значениями inDefine, inPragma, inError,
  inWarning, inCondition


