# Brief Description for Project Sources

- [Информация по исходным текстам](#информация-по-исходным-текстам)
- [Состав и содержимое исходников](#состав-и-содержимое-исходников)
  - [inc/umba](#incumba)
  - [inc/umba/tokenizer](#incumbatokenizer)
  - [inc/umba/tokenizer/filters](#incumbatokenizerfilters)
  - [inc/umba/tokenizer/lexers](#incumbatokenizerlexers)
  - [inc/umba/tokenizer/parsers/mermaid](#incumbatokenizerparsersmermaid)
  - [inc/umba/tokenizer/parsers](#incumbatokenizerparsers)
  - [inc/umba/tokenizer/parsers/ufsm](#incumbatokenizerparsersufsm)
  - [inc/umba/tokenizer/parsers/ufsm/impl](#incumbatokenizerparsersufsmimpl)
  - [inc/umba/tokenizer/parsers/ufsm](#incumbatokenizerparsersufsm-1)
  - [inc/umba/tokenizer/parsers/ufsm/samples](#incumbatokenizerparsersufsmsamples)
  - [inc/umba/tokenizer/parsers/ufsm](#incumbatokenizerparsersufsm-2)
  - [inc/umba/tokenizer/parsers](#incumbatokenizerparsers-1)
  - [inc/umba/tokenizer](#incumbatokenizer-1)
  - [inc/umba](#incumba-1)
- [Предупреждения](#предупреждения)
  - [inc/umba/tokenizer](#incumbatokenizer-2)
- [Обратить внимание](#обратить-внимание)
  - [inc/umba](#incumba-2)
  - [inc/umba/tokenizer](#incumbatokenizer-3)
  - [inc/umba/tokenizer/filters](#incumbatokenizerfilters-1)
  - [inc/umba/tokenizer/lexers](#incumbatokenizerlexers-1)
  - [inc/umba/tokenizer/parsers/mermaid](#incumbatokenizerparsersmermaid-1)
  - [inc/umba/tokenizer/parsers/ufsm](#incumbatokenizerparsersufsm-3)
  - [inc/umba/tokenizer/parsers/ufsm/impl](#incumbatokenizerparsersufsmimpl-1)
  - [inc/umba/tokenizer/parsers/ufsm/samples](#incumbatokenizerparsersufsmsamples-1)
- [Заметки](#заметки)
  - [inc/umba/tokenizer](#incumbatokenizer-4)
- [To do](#to-do)
  - [inc/umba/tokenizer](#incumbatokenizer-5)
  - [inc/umba/tokenizer/filters](#incumbatokenizerfilters-2)
  - [inc/umba/tokenizer/parsers/ufsm](#incumbatokenizerparsersufsm-4)
  - [inc/umba/tokenizer/parsers/ufsm/samples](#incumbatokenizerparsersufsmsamples-2)

# Информация по исходным текстам

# Состав и содержимое исходников


## inc/umba

- `c_char_class.h` - Константы классов символов. При помощи классов символов можно просто писать простейшие лексера. Уложены в 16 бит для компактности для встраиваемых применений.
- `c_char_class_table.h.inc` - Таблица классов символов для симолов базовой таблицы ASCII, версия "C"
- `c_tokenizer.h` - Утилиты для работы с trie символов. Предполагалось делать с поддержкой языка "C"
- `c_tokenizer_constants.h` - "Стандартные" предопределенные константы для токенов. Используются макросы, так как предполагается расширение набора констант, и предполагалось использование из языка "C"
- `char_class.h` - Константы классов символов (C++). При помощи классов символов можно просто писать простейшие лексера
- `char_class_table.h.inc` - Таблица классов символов для симолов базовой таблицы ASCII, версия "C++"
- `tokenizer.h` - Основной файл библиотеки токенизатора. Подключает все необходимые заголовки

## inc/umba/tokenizer

- `defs.h` - Общие макросы, а также определение/задание опций сборки. Внутренний хидер.
- `enums.h` - Autogenerated enumerations (Umba Enum Gen)
- `exceptions.h` - Исключения для marty::format. Стиль именов - snake_case, так как мы наследуемся от стандартной библиотеки

## inc/umba/tokenizer/filters

- `cc_preprocessor_filter.h` - Фильтр, вставляющий маркеры (без полезной нагрузки) препроцессора C/C++. Переключает режим угловых скобок, чтобы их содержимое парсилось как строка. Переключает режим символа '#'.
- `dbl_square_brackets_composing_filters.h` - Фильтры DblSquareBracketOpenComposingFilter и DblSquareBracketCloseComposingFilter - преобразуют двойные токены [[ и ]] в одинарные
- `filter_base.h` - Базовый класс для сложных фильтров с буферизацией токенов
- `identifier_to_keyword_conversion_filter.h` - Фильтр для преобразования идентификаторов в различные токены по заданной таблице
- `kebab_case_composing_filter.h` - Филтр для преобразования последовательностей идентификаторов и неразрывных с ними минусов в единственный kebab-style идентификатор
- `repeated_token_composing_filter.h` - Фильтр для преобразования простой последовательности из N токенов в единственный токен
- `simple_pass_trough_filter.h` - Фильтр, просто пропускающий токены дальше. Пример простого фильтра
- `simple_replace_filter.h` - Фильтр простой замены одного токена на другой. Нагрузка не меняется
- `simple_sequence_composing_filter.h` - Фильтр замены последовательности токенов на один токен
- `simple_suffix_gluing_filter.h` - Фильтр, "приклеивающий" идентификатор-суффикс к строковому или числовому литералу
- `token_collecting_filter.h` - Простой фильтр, который буферизирует одинаковые токены. Он хранит только итератор первого токена в последовательности
- `token_range_conversion_filter.h` - Конвертирует токены из диапазона в один токен, и устанавливает нагрузку в размер офсета от стартового, добавяя дополнительное смещение. Предназначено для операторов переменной длины
- `unclassified_chars_collecting_filter.h` - Фильтр, склеивающий отдельные неклассифицированные символы в последовательность

## inc/umba/tokenizer/lexers

- `cpp.h` - Лексер (сканер) для языка C/C++
- `mermaid_packet_diagram.h` - Лексер (сканер) диаграмм mermaid packet diagram
- `mermaid_packet_diagram_tokens.h` - Определение токенов, специфичных для mermaid packet diagram
- `plantuml.h` - Лексер (сканер) диаграмм plantuml
- `ufsm.h` - Лексер (сканер) диаграмм UmbaFSM
- `ufsm_tokens.h` - Определение токенов, специфичных для Umba State Machine
- `usketch_tokenizer.h` - Токенизатор/лексер usketch
- `usketch_tokens.h` - Определение токенов, специфичных для usketch

## inc/umba/tokenizer/parsers/mermaid

- `packet_diagram_cpp.h` - Хелперы для вывода в сишечку и плюсики, а также в текстовые диаграммы
- `packet_diagram_enums.h` - Autogenerated enumerations (Umba Enum Gen)
- `packet_diagram_parser.h` - Парсер mermaid packet diagram, расширенная версия, с использованием типов и массивов
- `packet_diagram_parser_types.h` - Типы для парсера mermaid packet diagram, расширенная версия, с использованием типов и массивов

## inc/umba/tokenizer/parsers

- `parser_base.h` - Базовый парсер
- `parser_base2.h` - Базовый парсер V2
- `parser_template.h` - Шаблон класса парсера

## inc/umba/tokenizer/parsers/ufsm

- `basic_typedefs.h` - Базовые typedef'ы для парсера ufsm
- `basic_types.h` - Базовые типы для парсера ufsm
- `enums.h` - Autogenerated enumerations (Umba Enum Gen)
- `exceptions.h` - Исключения для библиотеки ufsm
- `http.ufsm` -

## inc/umba/tokenizer/parsers/ufsm/impl

- `basic_types.h` - Базовые типы для парсера ufsm - реализация функций
- `types.h` - Типы для парсера ufsm - реализация функций

## inc/umba/tokenizer/parsers/ufsm

- `inserters.h` - Инсертеры - оператор вывода в поток
- `parser.h` - Парсер языка Umba FSM

## inc/umba/tokenizer/parsers/ufsm/samples

- `binr.ufsm` -
- `concept.ufsm` -
- `elevator.ufsm` -
- `http.ufsm` -
- `modbus.ufsm` -
- `nmea.ufsm` -
- `sirf.ufsm` -
- `traffic_lights.ufsm` -
- `trimble.ufsm` -

## inc/umba/tokenizer/parsers/ufsm

- `traffic_lights.ufsm` -
- `types.h` - Типы для парсера ufsm

## inc/umba/tokenizer/parsers

- `usketch_parser.h` - Парсер USketch - простое рисование
- `usketch_parser_types.h` - Типы для парсера usketch
- `utils.h` - Утилиты

## inc/umba/tokenizer

- `string_literal_parsing.h` - Интерфейс парсера строковых литералов ITokenizerLiteralParser, реализации парсеров SimpleQuotedStringLiteralParser и CppEscapedSimpleQuotedStringLiteralParser. Внутренний хидер.
- `token_collection.h` - Коллекция токенов.
- `token_filters.h` - Сборник всех фильтров токенизера
- `tokenizer.h` - Шаблонный класс Tokenizer, является typedef'ом для класса TokenizerFunctionHandlers, в котором используются обработчики на базе std::function. Внутренний хидер.
- `tokenizer_base.h` - Реализация токенизера. Внутренний хидер.
- `tokenizer_builder.h` - TokenizerBuilder - создание токенизера. Для создания поисковых trie в токенизере используются TrieBuilder'ы. Внутренний хидер.
- `tokenizer_function_handlers.h` - TokenizerFunctionHandlers - токенизатор с обработчиками std::function. Внутренний хидер.
- `tokenizer_log.h` - Подсистема протоколирования для токенизеров-парсеров
- `tokenizer_log_console.h` - Подсистема протоколирования для токенизеров-парсеров (вывод в консоль)
- `tokenizer_options.h` - TokenizerOptions - параметры работы токенизера. Внутренний хидер.
- `tpl.h` - Шаблон внутреннего хидера (не включаемого явно пользователем) библиотеки токенизатора. Внутренний хидер.
- `trie_builder.h` - TrieBuilder - построитель префиксного дерева trie. Для простоты хранит всё в map, при построении trie создаёт компактную структуру. Внутренний хидер.
- `types.h` - Типы данных для подсистемы токенизации
- `utils.h` - Утилиты. Внутренний хидер.

## inc/umba

- `undef_min_max.h` -


# Предупреждения


## inc/umba/tokenizer

- `[inc/umba/tokenizer/tokenizer_log.h:95]`
  /info report

- `[inc/umba/tokenizer/tokenizer_log.h:96]`
  /info report


# Обратить внимание


## inc/umba

- `[inc/umba/char_class.h:1345]`
  Сделать реверс последовательности для oO  пунктуации

- `[inc/umba/char_class.h:1514]`
  Тут ещё нужно добавить внешний цикл - текущий цикл становится внутренним, после
  него сортируем rangesStr, и опять прогоняем

- `[inc/umba/c_tokenizer_constants.h:76]`
  При изменении базовых констант не забываем, что нельзя вылезать за
  UMBA_TOKENIZER_TOKEN_BASE_LAST (или надо поправить эту константу)


## inc/umba/tokenizer

- `[inc/umba/tokenizer/string_literal_parsing.h:518]`
  В MSVC работает, в GCC надо через явное указание базы

- `[inc/umba/tokenizer/tokenizer_base.h:327]`
  Надо вспомнить, почему я сделал DataType##Holder, ведь изначально в variant'е
  были именно типы DataType Скорее всего, variant<DataType> был очень жирный, а
  также его копирование было дорогим. Но это не факт.

- `[inc/umba/tokenizer/tokenizer_base.h:785]`
  return'а не было, был забыт, а что и в каких случаях возращать - я уже не помню

- `[inc/umba/tokenizer/tokenizer_base.h:1244]`
  Тут надо преобразовать префикс в число, и поместить его в numberCurrentIntValue

- `[inc/umba/tokenizer/tokenizer_base.h:1261]`
  Не забыть передать numberCurrentIntValue

- `[inc/umba/tokenizer/tokenizer_base.h:1744]`
  Тут ещё надо проработать - а вдруг это 0 с каким-то суффиксом? по идее, всю
  цепочку итераторов надо бы хранить где-то, чтобы в данном случае сбросить её
  пользователю, а не выдавать ошибку

- `[inc/umba/tokenizer/tokenizer_base.h:1784]`
  тут надо уже начинать подсчитывать std::uint64_t numberCurrentIntValue

- `[inc/umba/tokenizer/tokenizer_base.h:1874]`
  Вот тут гавно какое-то получится, надо подумать

- `[inc/umba/tokenizer/tokenizer_base.h:1881]`
  Добавил

- `[inc/umba/tokenizer/tokenizer_base.h:1892]`
  Добавил

- `[inc/umba/tokenizer/tokenizer_base.h:1900]`
  unreachable code st = TokenizerInternalState::stReadNumberFloat;

- `[inc/umba/tokenizer/tokenizer_base.h:1914]`
  Вот тут гавно какое-то получится, надо подумать

- `[inc/umba/tokenizer/tokenizer_base.h:1948]`
  Вот тут гавно какое-то получится, надо подумать

- `[inc/umba/tokenizer/tokenizer_base.h:2383]`
  Пока IdentifierData задаем как вьюшку от итераторов, но вообще идентификатор
  надо бы сохранять в буфере, чтобы корректно обрабатывать linefeed escap'ы

- `[inc/umba/tokenizer/tokenizer_builder.h:223]`
  Чот какая-то херня непонятная сама ф-я addTokenToKnownSet выглядит так: void
  addTokenToKnownSet(payload_type tk, const string_type &tkStr, bool
  allowExisting=false) А тут мы string_type как-то очень странно конструируем.
  Вроде бы нам надо просто сделать строку из одного символа А тут мы вроде бы
  делаем строки размером bracketsPair[0] и bracketsPair[1] из символов
  pairBaseToken и pairBaseToken+1 Пока закоментим, и сделаем как надо
  addTokenToKnownSet(pairBaseToken  , string_type(bracketsPair[0], pairBaseToken
  )); addTokenToKnownSet(pairBaseToken+1, string_type(bracketsPair[1],
  pairBaseToken+1));

- `[inc/umba/tokenizer/token_collection.h:217]`
  У нас итераторы довольно жирные (32/64 байта), там овердофига всего хранится
  допустим, токенов у нас в 4 раза меньше, чем текста - если много кода, то много
  мелких токенов - операторов и тп но идентификаторы обычно длиннее 4х символов
  если есть коментарии или строковые литералы - то они разбавляют плотность

- `[inc/umba/tokenizer/token_collection.h:504]`
  Надо наверное что-то придумать с итератором конца. Или не надо? ,
  m_tokenizer(tknConfigurator(initTokenizerHandlers(std::move(tkn))))


## inc/umba/tokenizer/filters

- `[inc/umba/tokenizer/filters/simple_suffix_gluing_filter.h:138]`
  Зачем я тут копию делаю?


## inc/umba/tokenizer/lexers

- `[inc/umba/tokenizer/lexers/cpp.h:147]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/mermaid_packet_diagram.h:106]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/plantuml.h:178]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/ufsm.h:127]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/usketch_tokenizer.h:193]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/usketch_tokenizer.h:224]`
  Фильтры, установленные позже, отрабатывают раньше

- `[inc/umba/tokenizer/lexers/usketch_tokenizer.h:271]`
  Фильтры, установленные позже, отрабатывают раньше


## inc/umba/tokenizer/parsers/mermaid

- `[inc/umba/tokenizer/parsers/mermaid/packet_diagram_parser.h:1468]`
  Тут надо делать селект - если токен не числовой внутри орга, то литеральный
  глобальный Пока только числовой делаем

- `[inc/umba/tokenizer/parsers/mermaid/packet_diagram_parser.h:1490]`
  Надо разобраться, почему диагностика не туда пырит


## inc/umba/tokenizer/parsers/ufsm

- `[inc/umba/tokenizer/parsers/ufsm/http.ufsm:44]`
  Или таблица на 256 символов? Лучше так, да. - для каждого "символьного"
  предиката генерируем соответствующий флаг; - заполняем таблицу флагами,
  индексом служит код символа; - нужно как-то получить информацию, где брать код
  символа. Есть варик генерить, описал выше. Тогда если предикат символьный, то
  char берём оттуда; - для каждого "символьного" предиката генерим предикатную
  функцию. Предикатная функция извлекает флаги из массива флагов, и проверяет
  наличие во флагах "своего" флага.

- `[inc/umba/tokenizer/parsers/ufsm/parser.h:44]`
  В definitions нельзя добавлять переходы. Сейчас это не проверяется

- `[inc/umba/tokenizer/parsers/ufsm/parser.h:771]`
  Дублирование кода, вынести в отдельную лямбду

- `[inc/umba/tokenizer/parsers/ufsm/parser.h:1304]`
  Нужно проверить наличие этого флага, если уже установлен, то это ошибка Пока
  просто устанавливаем без проверки


## inc/umba/tokenizer/parsers/ufsm/impl

- `[inc/umba/tokenizer/parsers/ufsm/impl/types.h:294]`
  Данная версия expandTransitions нужна для реализации автомата Для рисования
  графа нам не нужно раскрывать events - граф будет сильно замусорен, но нам
  нужно раскрывать sourceStates - так как рисовать события нужно для каждой
  вершины. При этом нам нужно как-то помечать раскрытые по sourceStates переходы
  каким-то ID для того, чтобы проверить, присутствует ли что-либо с таким ID в
  полностью раскрытом графе. Если не присутствует, то такой нераскрытый по events
  переход не отображается.


## inc/umba/tokenizer/parsers/ufsm/samples

- `[inc/umba/tokenizer/parsers/ufsm/samples/http.ufsm:46]`
  Или таблица на 256 символов? Лучше так, да. - для каждого "символьного"
  предиката генерируем соответствующий флаг; - заполняем таблицу флагами,
  индексом служит код символа; - нужно как-то получить информацию, где брать код
  символа. Есть варик генерить, описал выше. Тогда если предикат символьный, то
  char берём оттуда; - для каждого "символьного" предиката генерим предикатную
  функцию. Предикатная функция извлекает флаги из массива флагов, и проверяет
  наличие во флагах "своего" флага.


# Заметки


## inc/umba/tokenizer

- `[inc/umba/tokenizer/tokenizer_base.h:882]`
  Надо ли semialpha проверять, не является ли она началом числового префикса?
  Наверное, не помешает

- `[inc/umba/tokenizer/tokenizer_base.h:1698]`
  У нас пока так: префикс числа начинается с любой цифры, потом могут следовать
  любые символы, после префикса - те символы, которые разрешены префиксом

- `[inc/umba/tokenizer/tokenizer_base.h:1780]`
  Да, сразу после префикса у нас не может быть разделителя разрядов


# To do


## inc/umba/tokenizer

- [ ] `[inc/umba/tokenizer/tokenizer_base.h:579]`
  Надо определится, что делать в tokenize в raw режиме. См там коментарии

- [ ] `[inc/umba/tokenizer/tokenizer_base.h:1280]`
  У нас был встречен символ разделяющий целую и дробную часть плавающего числа,
  но без целой части. Пока просто съедаем его, но вообще этот символ может быть
  оператором, надо это обработать Например, это может быть паскалевский `End.`
  (End с точкой)

- [ ] `[inc/umba/tokenizer/tokenizer_base.h:1319]`
  Надо уточнить, что за комент

- [ ] `[inc/umba/tokenizer/tokenizer_base.h:2093]`
  Разобраться с continuation

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:216]`
  Проверить, что pairBaseToken один из: UMBA_TOKENIZER_TOKEN_CURLY_BRACKETS
  UMBA_TOKENIZER_TOKEN_ROUND_BRACKETS UMBA_TOKENIZER_TOKEN_ANGLE_BRACKETS
  UMBA_TOKENIZER_TOKEN_SQUARE_BRACKETS или 0, для автоопределения, и сделать
  автоопределение

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:253]`
  Подумать, как сделать, чтобы числа можно было начинать с символов @ # $

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:254]`
  Проверить tokenId на вхождение в диапазон, или сделать автоопределение

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:271]`
  Проверить tokenId на вхождение в диапазон, или сделать автоопределение

- [ ] `[inc/umba/tokenizer/tokenizer_builder.h:315]`
  Проверить tokenId на вхождение в диапазон, или сделать автоопределение

- [ ] `[inc/umba/tokenizer/tokenizer_log.h:163]`
  Тут надо бы сделать итерацию по символам текста, а не по char'ам

- [ ] `[inc/umba/tokenizer/token_collection.h:155]`
  Ещё забыт вертикальный TAB

- [ ] `[inc/umba/tokenizer/token_collection.h:156]`
  Символы меньше пробела - сейчас по идее, просто пропускаются, наверное надо
  сделать опцию, чтобы они вызывали ошибку

- [ ] `[inc/umba/tokenizer/token_collection.h:268]`
  Надо переделать на unordered_map, чтобы можно было удалять элементы

- [ ] `[inc/umba/tokenizer/token_collection.h:298]`
  запретить все остальные конструкторы

- [ ] `[inc/umba/tokenizer/token_collection.h:299]`
  запретить копирование

- [ ] `[inc/umba/tokenizer/token_collection.h:764]`
  может не надо в getTokenImpl проверять доступность? Или пофик, одно условие,
  зато peekToken() упрощается

- [ ] `[inc/umba/tokenizer/token_collection.h:839]`
  Вот тут надо вычитать следующий токен из входного текста

- [ ] `[inc/umba/tokenizer/token_collection.h:886]`
  надо как-то просигналить, потому что снаружы мы генерим только ошибки
  синтаксического анализа а ошибки лексера генерируются им самим. Но раз лексер
  вернул true, то там ошибки не было, но, тем не менее, финальный токен не
  появился в массиве токенов - что-то пошло не так. unexpected-что?


## inc/umba/tokenizer/filters

- [ ] `[inc/umba/tokenizer/filters/cc_preprocessor_filter.h:42]`
  Вообще, по уму, надо бы сделать энум со значениями inDefine, inPragma, inError,
  inWarning, inCondition


## inc/umba/tokenizer/parsers/ufsm

- [ ] `[inc/umba/tokenizer/parsers/ufsm/http.ufsm:30]`
  Нужно реализовать разбор таких строк Примеры: isAsciiPrnChar  =
  "\a![\x00-\x20]" - все символы базовой ASCII (_md/7 бит), исключая диапазон от нуля
  до пробела isAnyPrnChar    = "\*![\x00-\x20]" - все символы (_md/8 бит), исключая
  диапазон от нуля до пробела isDigit         = "0-9" isDigit         = "\d"
  isHexDigit      = "\da-fA-f" isAlphaUpper    = "A-Z"; isAlphaLower    = "a-z";
  isAlpha = isAlphaUpper | isAlphaLower;


## inc/umba/tokenizer/parsers/ufsm/samples

- [ ] `[inc/umba/tokenizer/parsers/ufsm/samples/http.ufsm:32]`
  Нужно реализовать разбор таких строк Примеры: isAsciiPrnChar  =
  "\a![\x00-\x20]" - все символы базовой ASCII (_md/7 бит), исключая диапазон от нуля
  до пробела isAnyPrnChar    = "\*![\x00-\x20]" - все символы (_md/8 бит), исключая
  диапазон от нуля до пробела isDigit         = "0-9" isDigit         = "\d"
  isHexDigit      = "\da-fA-f" isAlphaUpper    = "A-Z"; isAlphaLower    = "a-z";
  isAlpha = isAlphaUpper | isAlphaLower;


